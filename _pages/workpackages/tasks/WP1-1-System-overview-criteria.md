---
permalink: /workpackages/tasks/WP1-1-System-overview-criteria/
title: "WP 1.1 (Task 001): System Overview Criteria"
layout: splash
---

---

# WP 1.1 (Task 001): System Overview Criteria
## Importance:
Essential
## Potential RTP groups for this task:
Open to whole consortium

## Description:
There are no guidelines of what a good system description should look like. Most 
descriptions at least enlist the type of hardware and how to gain access, but the level of 
detail is different for a lot of systems. This Task shall collect ideas from various places 
and consortium members (e.g. conduct interviews) of what a good testbed 
description looks like. It is expected that the level of detail will be different for different systems, but we expect these to consider:

- Available Hardware
	- Number of compute nodes, cores per node, and their specifications (e.g., 
CPU model, clock speed, memory capacity, and type).
	- Storage details (e.g., type, capacity, bandwidth).
	- Network details (e.g., topology, interconnect type, bandwidth, latency).

- Available Software
	- Operating system and its version.
	- Installed compilers (e.g., Intel, GCC) and their versions.
	- Installed libraries (e.g., MPI, math libraries) and their versions.
	- Job scheduler details (e.g., Slurm, PBS)
- Resource Management
	- Job scheduling policies and resource limits.
	- Accounting and usage policies.
- Access and availability
	- Policies and instructions on how to obtain access to the system.
	
The details above should thus act as prompts for consortium members to provide their 
opinion on:
- What information do we expect testbeds to provide, i.e. what is the minimum info?
- Can we provide some examples of really good system descriptions? Would a good 
system description for example provide some information on the precise 
hardware topology such as cache topology?
-  What information beyond the system description should a testbed page offer? For 
example, do we expect a good site to present data for a certain type of benchmark 
(e.g. Stream, MPI tests, ...)?

The Task should collect all of this information into a checklist which is published on the 
SHAREing webpage. It should also add one or two examples to the SHAREingpage, i.e. 
take an arbitrary testbed available to the consortium and create a showcase 
documentation for this particular system.






## Outcome/acceptance:

- Conduct a consultation through a series of 1:1 interviews with consortium 
members on what they expect a good webpage to provide in terms of information.
- Submitted Pull Requests to SHAREing webpage that includes
	- A new page that describes what a good testbed looks like.
	- Examples of one or two testbeds that showcase what a good description 
should look like

[Apply now](https://www.linkedin.com/company/shareing/?lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_all%3BFDgR8reHROmSyuRt0XDgaQ%3D%3D){: .btn .btn--info .btn--large}

